{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654c561c-b39f-40e5-8070-4ca398db7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e4b3d3-7199-4d58-ab38-28953b2ae39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_gz_file(filepath):\n",
    "    with gzip.open(filepath, 'rt', encoding='utf-8') as file:\n",
    "        print(filepath)\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data\n",
    "def concatenate_json_files(directory):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json.gz\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            file_data = load_json_gz_file(filepath)\n",
    "            all_data.extend(file_data)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1711235-2433-4474-88f8-21177a31217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  directory containing the JSON files\n",
    "directory = './genre'\n",
    "\n",
    "# Concatenating all JSON files into a single DataFrame\n",
    "df = concatenate_json_files(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574e15e-87fa-4f89-b9d9-485a4b7a3cb3",
   "metadata": {},
   "source": [
    "### This method took a LOT of time so we had to process by chunks and save multiple dataframes that will later merged and cleaned ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f2452-ae1b-495f-8743-30c3624812a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Temp\\ipykernel_10556\\3470257456.py:42: DtypeWarning: Columns (7,29,30,31,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_dfs.append(pd.read_csv(filename))\n",
      "C:\\Users\\Sara\\AppData\\Local\\Temp\\ipykernel_10556\\3470257456.py:42: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_dfs.append(pd.read_csv(filename))\n",
      "C:\\Users\\Sara\\AppData\\Local\\Temp\\ipykernel_10556\\3470257456.py:42: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_dfs.append(pd.read_csv(filename))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def process_json_gz_file(filepath, chunk_size=10000):\n",
    "    with gzip.open(filepath, 'rt', encoding='utf-8') as file:\n",
    "        chunk = []\n",
    "        for i, line in enumerate(file):\n",
    "            chunk.append(json.loads(line))\n",
    "            if (i + 1) % chunk_size == 0:\n",
    "                yield pd.DataFrame(chunk)\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield pd.DataFrame(chunk)\n",
    "\n",
    "def save_intermediate_csv_files(directory, output_prefix, chunk_size=10000, batch_size=10):\n",
    "    batch_data = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json.gz\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            for chunk_df in process_json_gz_file(filepath, chunk_size):\n",
    "                batch_data.append(chunk_df)\n",
    "                \n",
    "                if len(batch_data) >= batch_size:\n",
    "                    combined_df = pd.concat(batch_data, ignore_index=True)\n",
    "                    combined_df.to_csv(f\"{output_prefix}_{batch_count}.csv\", index=False)\n",
    "                    batch_data = []\n",
    "                    batch_count += 1\n",
    "\n",
    "    # Saving any remaining data\n",
    "    if batch_data:\n",
    "        combined_df = pd.concat(batch_data, ignore_index=True)\n",
    "        combined_df.to_csv(f\"{output_prefix}_{batch_count}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8e41c6-84ac-4b06-ab4f-1ea6abc97331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65472b-5d2f-4c68-80bc-8187b06b9602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bc887-01f1-403d-be51-d6bbfc67b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_all_csv_files(output_prefix, final_output_file):\n",
    "    all_dfs = []\n",
    "    for filename in os.listdir('.'):\n",
    "        if filename.startswith(output_prefix) and filename.endswith(\".csv\"):\n",
    "            all_dfs.append(pd.read_csv(filename))\n",
    "    \n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df.to_csv(final_output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Intermediate and final output file prefixes\n",
    "output_prefix = 'intermediate_goodreads_data'\n",
    "final_output_file = 'final_concatenated_goodreads_data.csv'\n",
    "\n",
    "\n",
    "# Concatenating all intermediate CSV files into one final CSV file\n",
    "concatenate_all_csv_files(output_prefix, final_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a8b009-6f65-4161-b3f1-20971cd3e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Temp\\ipykernel_1852\\3516056252.py:5: DtypeWarning: Columns (7,29,30,31,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_dfs.append(pd.read_csv(filename).head(2000))\n",
      "C:\\Users\\Sara\\AppData\\Local\\Temp\\ipykernel_1852\\3516056252.py:5: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_dfs.append(pd.read_csv(filename).head(2000))\n",
      "C:\\Users\\Sara\\AppData\\Local\\Temp\\ipykernel_1852\\3516056252.py:5: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_dfs.append(pd.read_csv(filename).head(2000))\n"
     ]
    }
   ],
   "source": [
    "## Finally this was just for testing purposes since the csv were heavy we tried to take just 2000 of each at the end from 14 csv we just took 6\n",
    "\n",
    "def concatenate_all_csv_files(output_prefix, final_output_file):\n",
    "    all_dfs = []\n",
    "    for filename in os.listdir('.'):\n",
    "        if filename.startswith(output_prefix) and filename.endswith(\".csv\"):\n",
    "            all_dfs.append(pd.read_csv(filename).head(2000))\n",
    "    \n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df.to_csv(final_output_file, index=False)\n",
    "\n",
    "\n",
    "output_prefix = 'intermediate_goodreads_data'\n",
    "final_output_file = 'final_concatenated_goodreads_data_small.csv'\n",
    "\n",
    "concatenate_all_csv_files(output_prefix, final_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9c05b-ab13-4ae7-b6b4-f650524589f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
